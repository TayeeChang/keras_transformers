# Keras Transformer Family

![Authour](https://img.shields.io/badge/Author-Tayee%20Chang-blue.svg) 
![Python](https://img.shields.io/badge/Python-3.6+-brightgreen.svg)
![Tensorflow](https://img.shields.io/badge/TensorFlow-1.13.0-yellowgreen.svg)
![NLP](https://img.shields.io/badge/NLP-Transformers-redgreen.svg)
[![License](https://img.shields.io/badge/License-Apache--2.0-green.svg)](https://github.com/TayeeChang/transformers/blob/master/LICENSE)


ğŸš€ğŸš€ğŸš€  Transformerå®¶æ—æ¨¡å‹Kerasç‰ˆå®ç°ï¼Œå¯åŠ è½½å®˜æ–¹é¢„è®­ç»ƒæƒé‡æ¥æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚

ç›®å‰æ”¯æŒçš„Transformeræ¨¡å‹ï¼š   
- BERTâ€”â€”[ä¸‹è½½](https://github.com/google-research/bert)  
- Robertaâ€”â€”[ä¸‹è½½|brightmartç‰ˆ](https://github.com/brightmart/roberta_zh) [ä¸‹è½½|å“ˆå·¥å¤§ç‰ˆ](https://github.com/ymcui/Chinese-BERT-wwm)
- Albertâ€”â€”[ä¸‹è½½](https://github.com/brightmart/albert_zh)  
- Nezhaâ€”â€”[ä¸‹è½½](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow)   
- Unilmâ€”â€”[ä¸‹è½½](https://github.com/google-research/bert)  
- Electraâ€”â€”[ä¸‹è½½|googleç‰ˆ](https://github.com/google-research/electra) [ä¸‹è½½|å“ˆå·¥å¤§ç‰ˆ](https://github.com/ymcui/Chinese-ELECTRA)
- GPTâ€”â€”[ä¸‹è½½](https://github.com/bojone/CDial-GPT-tf)
- GPT2â€”â€”[ä¸‹è½½](https://github.com/imcaspar/gpt2-ml)
- T5.1.1â€”â€”[ä¸‹è½½](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)  
- T5â€”â€”[ä¸‹è½½](https://github.com/google-research/text-to-text-transfer-transformer)

åç»­ä¼šé™†ç»­æ·»åŠ å…¶ä»–Transformeræ¨¡å‹ã€‚

ç»§ç»­å®Œå–„ä¸­...

æ¬¢è¿ä½¿ç”¨

## è¯´æ˜

   ç¯å¢ƒä½¿ç”¨  
   - keras >= 2.3.1  
   - tensorflow == 1.13 or 1.14 (å»ºè®®)
   
## å®‰è£…
```shell   
pip install git+https://github.com/TayeeChang/transformers.git
```
æˆ–è€…
```shell
python setup.py install
```

## ä½¿ç”¨
 
 å…·ä½“å‚è€ƒ [example](https://github.com/TayeeChang/transformers/tree/master/example)
 
## å¼•ç”¨
1. <a href="https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
2. <a href="https://arxiv.org/pdf/1907.11692.pdf%5C">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
3. <a href="https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a>
4. <a href="https://arxiv.org/pdf/1909.00204.pdf">NEZHA: NEURAL CONTEXTUALIZED REPRESENTATION FOR CHINESE LANGUAGE UNDERSTANDING</a>
5. <a href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a>
6. <a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>
7. <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving Language Understanding by Generative Pre-Training</a>
8. <a href="http://www.persagen.com/files/misc/radford2019language.pdf">Language Models are Unsupervised Multitask Learners</a>
9. <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
10. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>
11. <a href="https://github.com/CyberZHG/keras-bert">https://github.com/CyberZHG/keras-bert</a>
